
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>CFP: 3rd CEFRL International Workshop@CVPR 2019 “Compact and Efficient Feature Representation and Learning in Computer Vision”</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="CEFRLatCVPR2019/css/style.css" rel="stylesheet" type="text/css" />
</head>

<body> 

<div class="container">
  <p><img src="CEFRLatCVPR2019/figures/title2.png" width="1000" align="middle" /></p>
</div>

</br>

<div class="container">
  <h2>Invited Speakers</h2>
    <div>
      <table align="center">
        <td>
      <div class="instructor">
        <a href="https://www.cs.princeton.edu/~jiadeng/">
            <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/jia.png"></div>
            <div>Dr. Jia Deng<br>Princeton University, United States</div>
        </a>
      </div>
        </td>

        <td>
      <div class="instructor">
          <a href="https://jingdongwang2017.github.io/" >
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/jingdong.png"></div>
        <div>Dr. Jingdong Wang<br>Microsoft Research, Beijing, China</div>
        </a>
      </div>
        </td>
      </table>

    </div>
    <p></p> 
</div>   

</br>

<div class="container">
    <div class="overview">
    <p><strong>Title: </strong>Learning Single-Image 3D Representations</p>
    <p><strong>Speaker: </strong>Dr. Jia Deng</p>
    <p><strong>Bio: </strong>Jia Deng is an Assistant Professor of Computer Science at Princeton University. He received his Ph.D. from Princeton University and his BSc degree from Tsinghua University, both in computer science. He is a recipient of the Sload Research Fellowship, the PAMI Mark Everingham Prize, the Yahoo ACE Award, a Google Faculty Research Award, the ICCV Marr Prize, and the ECCV Best Paper Award. His research focus is on computer vision and machine learning. His papers have currently over 19,000 citations in Google Scholar.</p>
    </div>
</div>

<div class="container">
    <div class="overview">
    <p><strong>Title: </strong>Deep high-resolution representation learning for visual recognition</p>
    <p><strong>Speaker: </strong>Dr. Jingdong Wang</p>
    <p><strong>Abstract: </strong>Classification networks have been dominant in visual recognition, from image-level classification to region-level classification (object detection) and pixel-level classification (semantic segmentation, human pose estimation, and facial landmark detection). We argue that the classification network, formed by connecting high-to-low convolutions in series, is not a good choice for region-level and pixel-level classification because it only leads to rich low-resolution representations or poor high-resolution representations obtained with upsampling processes.</p>
    <p>We propose a high-resolution network (HRNet). The HRNet maintains high-resolution representations by connecting high-to-low resolution convolutions in parallel and strengthens high-resolution representations by repeatedly performing multi-scale fusions across parallel convolutions. We demonstrate the effectives on pixel-level classification, region-level classification, and image-level classification. The HRNet turns out to be a strong repalcement of classification networks (e.g., ResNets, VGGNets) for visual recognition.</p>
    <p><strong>Bio: </strong>Jingdong Wang is a Senior Researcher with the Visual Computing Group, Microsoft Research, Beijing, China. His areas of current interest include CNN architecture design, human pose estimation, semantic segmentation, person re-identification, large-scale indexing, and salient object detection. He has authored one book and 100+ papers in top conferences and prestigious international journals in computer vision, multimedia, and machine learning. He authored a comprehensive survey on learning to hash in TPAMI. His paper was selected into the Best Paper Finalist at the ACM MM 2015. Dr. Wang is an Associate Editor of IEEE TPAMI, IEEE TCSVT and IEEE TMM. He was an Area Chair or a Senior Program Committee Member of top conferences, such as CVPR, ICCV, ECCV, AAAI, IJCAI, and ACM Multimedia. He is an ACM Distinguished Member and a Fellow of the IAPR. His homepage is <a href="https://jingdongwang2017.github.io">https://jingdongwang2017.github.io</a></p>
    </div>
</div>

<br>

<div class="container">
  <h2>Overview</h2>
    <div class="overview">
    <p>Feature representation is at the core of many computer vision and pattern recognition applications such as image classification, object detection, image and video retrieval, image matching and many others. For years, milestone engineered feature descriptors such as Scale Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) have dominated various domains of computer vision. The design of feature descriptors with low computational complexity has gained lots of attention and a number of efficient descriptors including BRIEF, FREAK, BRISK and DAISY have been presented. In the past few years we have witnessed significant progress in feature representation and learning. The popularity of traditional handcrafted features seems to be overtaken by the Deep Convolutional Neural Networks (DeepCNNs), which can learn powerful features automatically from data and have brought about breakthroughs in various problems in computer vision. However, these advances rely on deep networks with millions or even billions of parameters, and the availability of GPUs with very high computation capability and large scale labeled datasets plays a key role in their success. In other words, powerful DeepCNNs are data hungry and energy hungry. </p>
    <p>Nowadays, featuring exponentially increasing number of images and videos, the emerging phenomenon of big dimensionality (millions of dimensions and above) renders the inadequacies of existing approaches, no matter traditional handcrafted features or recent deep learning based ones. There is thus a pressing need for new scalable and efficient approaches that can cope with this explosion of dimensionality. In addition, with the prevalence of social media networks and portable/mobile/wearable devices which have limited resources (e.g. battery life, memory, storage space, CPUs, and bandwidth), the demands for sophisticated portable/mobile/wearable device applications in handling large-scale visual data is rising. In such applications, real time performance is of utmost importance to users, since no one is willing to spend any time waiting nowadays. Therefore, there is a growing need for feature descriptors that are fast to compute, memory efficient, and yet exhibiting good discriminability and robustness. A number of attempting efforts, such as compact binary features, DCNN network quantization, simple and efficient neural network architectures and big dimensionality-oriented feature selection, have appeared in top conferences (including CVPR, ICCV, ECCV, NIPS and ICLR) and top journals (including TPAMI and IJCV). The aim of this workshop is to stimulate researchers from the fields of computer vision to present high quality work and to provide a cross-fertilization ground for stimulating discussions on the next steps in this important research area.</p>
    </div>
</div>

<br> 

<div class="container">
  <h2 align='center'>Important Dates(Tentative)</h2>
  <table border="2" align="center">
    <tr>
      <td width="300"><htable>Event</htable></td> <td width="300"><htable>Date</htable></td>
    </tr>
    <tr>
      <td>Paper Submission Deadline</td><td><strong>March 24, 2019</strong></td>
    </tr>
    <tr>
      <td>Notification of Acceptance</td><td><strong>April 6, 2019</strong></td>
    </tr>
    <tr>
      <td>Camera-ready due</td><td><strong>April 18, 2019</strong></td>
    </tr>
    <tr>
      <td>Workshop (Half day)</td><td><strong>June 16, 2019 (pm)</strong></td>
    </tr>
  </table>
  <p></p>
  <p></p>
</div>

</br>

<div class="container">
  <h2>Topics</h2>
    <div class="topic">
    <p>We encourage researchers to study and develop new compact and efficient feature representations that are fast to compute, memory efficient, and yet exhibiting good discriminability and robustness. We also encourage new theories and applications related to feature representation and learning for dealing with these challenges. We are soliciting original contributions that address a wide range of theoretical and practical issues including, but not limited to:</p>
    <p>1. New features (handcrafted features, lightweight DeepCNN architectures, deep model compression/quantization, and feature learning in supervised, weakly supervised or unsupervised way) that are fast to compute, memory efficient and suitable for large scale problems;</p>
    <p>2. New compact and efficient features that are suitable for wearable devices (e.g., smart glasses, smart phones, smart watches) with strict requirements for computational efficiency and low power consumption;</p>
    <p>3. Hashing/binary codes learning and its related applications in different domains, e.g. content based retrieval;</p>
    <p>4. Evaluations of current traditional descriptors and features learned by deep learning;</p>
    <p>5. Hybrid methods combining strengths of handcrafted and learning based approaches;</p>
    <p>6. New applications of existing features in different domains, e.g. medical domain;</p>
    </div>
</div>

</br>

<div class="container">
  <h2>Program outline (afternoon, 16 June 2019, Hyatt Shoreline B)</h2>
  <table border="2" align="center">
    <tr>
      <td width="200"><htable>Time</htable></td> <td width="400"><htable>Event</htable></td>
    </tr>
    <tr>
      <td><strong>13:50~14:00</strong></td><td>Welcome Introduction</td>
    </tr>
    <tr>
      <td><strong>14:00~14:45</strong></td><td><strong>Invited Talk (Jia Deng)</strong></td>
    </tr>
    <tr>
      <td><strong>14:45~15:25</strong></td><td><strong>Oral Session 1</strong></td>
    </tr>
    <tr>
      <td><strong>15:25~16:25</strong></td><td><strong>Poster Session</strong></td>
    </tr>
    <tr>
      <td><strong>16:25~17:10</strong></td><td><strong>Invited Talk (Jingdong Wang)</strong></td>
    </tr>
    <tr>
      <td><strong>17:10~17:50</strong></td><td><strong>Oral Session 2</strong></td>
    </tr>
    <tr>
      <td><strong>17:50~18:00</strong></td><td>Closing Remarks</td>
    </tr>
  </table>
  <p></p>
  <p></p>
</div>

</br>

<div class="container" text-align="left">
  <h3>Oral Session 1 (14:45~15:25)</h3>
    <div class="topic">
      <ul> 
        <li><strong>Weakly Supervised Object Discovery by Generative Adversarial & Ranking Networks</strong>, Ali Diba, Vivek Sharma, Rainer Stiefelhagen, Luc Van Gool</li>
        <li><strong>Deep Anchored Convolutional Neural Networks</strong>, Jiahui Huang, Kshitij Dwivedi, Gemma Roig</li>
      </ul>
    </div>
    <br>
  <h3>Oral Session 2 (17:10~17:50)</h3>
    <div class="topic">
      <ul>
        <li><strong>Efficient Super Resolution Using Binarized Neural Network</strong>, Yinglan Ma, Hongyu Xiong, Zhe Hu, Lizhuang Ma</li>
        <li><strong>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</strong>, Youngwan Lee, joong-won hwang, Sangrok Lee,; Yuseok Bae, Jongyoul Park</li>
      </ul>
    </div>
    <br>
  <h3>Poster Session (15:25~16:25)</h3>
    <div class="topic">
      <ul>
        <li><strong>Robust Visual Tracking via Collaborative and Reinforced Convolutional Feature Learning</strong>, Dongdong Li, Yangliu Kuai, Gongjian Wen, Li Liu</li>
        <li><strong>Weakly Supervised Object Discovery by Generative Adversarial & Ranking Networks</strong>, Ali Diba, Vivek Sharma, Rainer Stiefelhagen, Luc Van Gool</li>
        <li><strong>Video-Based Action Recognition Using Dimension Reduction of Deep Covariance Trajectories</strong>, Mengyu Dai, Anuj Srivastava</li>
        <li><strong>Adaptive Labeling for Deep Learning to Hash</strong>, Huei-Fang Yang, Cheng-Hao Tu, Chu-Song Chen</li>
        <li><strong>Deep Anchored Convolutional Neural Networks</strong>, Jiahui Huang, Kshitij Dwivedi, Gemma Roig</li>
        <li><strong>Knowledge Representing: Efficient, Sparse Representation of Prior Knowledge for Knowledge Distillation</strong>, Junjie Liu, Dongchao Wen, Hongxing Gao, Wei Tao, Tse-Wei Chen, Kinya Osa, Masami Kato</li>
        <li><strong>Single Image Based Metric Learning via Overlapping Blocks Model for Person Re-Identification</strong>, Yipeng Chen, cairong zhao, Tianli Sun</li>
        <li><strong>Class Consistency Driven Unsupervised Deep Adversarial Domain Adaptation</strong>, Sayan Rakshit, Ushasi Chaudhuri, Biplab Banerjee, Subhasis Chaudhuri</li>
        <li><strong>Dynamic Representations Toward Efficient Inference on Deep Neural Networks by Decision Gates</strong>, Mohammad Saeed Shafiee, Mohammad Javad Shafiee, Alexander Wong</li>
        <li><strong>Compact Scene Graphs for Layout Composition and Patch Retrieval</strong>, Subarna Tripathi, Sairam Sundaresan, Sharath Nittur Sridhar, Hanlin Tang</li>
        <li><strong>AttoNets: Compact and Efficient Deep Neural Networks for the Edge via Human-Machine Collaborative Design</strong>, Alexander Wong, Zhong Qiu Lin, Brendan Chwyl</li>
        <li><strong>Efficient Super Resolution Using Binarized Neural Network</strong>, Yinglan Ma, Hongyu Xiong, Zhe Hu, Lizhuang Ma</li>
        <li><strong>Generative Model for Zero-Shot Sketch-Based Image Retrieval</strong>, Vinay Kumar Verma, AAKANSHA MISHRA, Ashish Mishra, Piyush Rai</li>
        <li><strong>Efficient Deep Palmprint Recognition via Distilled Hashing Coding</strong>, Huikai Shao, Dexing Zhong, Xuefeng Du</li>
        <li><strong>Image Denoising Using Deep CGAN with Bi-skip Connections</strong>, Peng Wang</li>
        <li><strong>Pairwise Teacher-Student Network for Semi-Supervised Hashing</strong>, Shifeng Zhang, Jianmin Li, Bo Zhang</li>
        <li><strong>A site model based change detection method for SAR images</strong>, Wei Wang, Jianhua Shi, Lingjun Zhao, Xingwei Yan</li>
        <li><strong>Salient Object Detection in Low Contrast Images via Global Convolution and Boundary Refinement</strong>, Nan Mu, Xin Xu, Xiaolong Zhang</li>
        <li><strong>An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</strong>, Youngwan Lee, joong-won hwang, Sangrok Lee,; Yuseok Bae, Jongyoul Park</li>
        <li><strong>Scan-flood Fill(SCAFF): an Efficient Automatic Precise Region Filling Algorithm for Complicated Regions</strong>, Yixuan He, Tianyi Hu, Delu Zeng</li>
      </ul>
    </div>
</div>

</br>

<div class="container" text-align="left">
  <h2>Paper Submission Information</h2>
    <div class="topic">
      <p>All submissions will be handled electronically via the workshop’s CMT Website.  Click the following link to go to the submission site: <a href="https://cmt3.research.microsoft.com/CEFRL2019">https://cmt3.research.microsoft.com/CEFRL2019</a></p>
      <p>Papers should describe original and unpublished work about the related topics. Each paper will receive double blind reviews, moderated by the workshop chairs. Authors should take into account the following:</p> 
      <p>- All papers must be written and presented in English.</p> 
      <p>- All papers must be submitted in PDF format. The workshop paper format guidelines are the same as the <a href="http://cvpr2019.thecvf.com/submission/main_conference/author_guidelines#rebuttal_instructions">Main Conference papers</a></p> 
      <p>- The maximum paper length is 8 pages (excluding references). Note that shorter submissions are also welcome.</p> 
      <p>- The accepted papers will be published in CVF open access as wel as in IEEE Xplore.</p> 
    </div>
</div>

</br>

<div class="container">
  <h2>Organizers</h2>
    <div>

      <div class="instructor">
          <a href="http://www.ee.oulu.fi/~lili/LiLiuHomepage.html">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/li.png"></div>
        <div>Dr. Li Liu</div>
        </a>
        <div>(University of Oulu & NUDT)</div>
      </div>

      <div class="instructor">
          <a href="https://scholar.google.com/citations?user=WQRNvdsAAAAJ&hl=zh-CN">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/yulan.png"></div>
        <div>Dr. Yulan Guo</div>
        </a>
        <div>(NUDT)</div>
      </div>

      <div class="instructor">
          <a href="https://scholar.google.com/citations?user=pw_0Z_UAAAAJ&hl=zh-CN">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/wanli.png"></div>
        <div>Dr. Wanli Ouyang</div>
        </a>
        <div>(Univeristy of Sydney)</div>
      </div>

      <br> 
      <p></p>

      <div class="instructor">
          <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=zh-CN">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/jiwen.png"></div>
        <div>Dr. Jiwen Lu</div>
        </a>
        <div>(Tsinghua University)</div>
      </div>

      <div class="instructor">
          <a href="https://scholar.google.com/citations?user=bjEpXBoAAAAJ&hl=zh-CN">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/matti.png"></div>
        <div>Prof. Matti Pietikäinen</div>
        </a>
        <div>(University of Oulu)</div>
      </div>

      <div class="instructor">
          <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=zh-CN">
        <div class="instructorphoto"><img width="200" src="CEFRLatCVPR2019/figures/luc.png"></div>
        <div>Prof. Luc Van Gool</div>
        </a>
        <div>(ETH Zurich)</div>
      </div>

    </div>
    <p></p> 
</div>   

</br>

<div class="container">
  <h2>Previous CEFRL Workshop</h2>
    <div class="history">
      <h3>
      <p>· <a href="https://cefrl.webflow.io/">2nd CEFRL Workshop in conjunction with ECCV 2018</a></p>
      <p>· <a href="http://www.ee.oulu.fi/~lili/ICCVW2017.html">1st CEFRL Workshop in conjunction with ICCV 2017</a></p>
      </h3>
    </div>
</div>

</br>

<div class="containersmall">
    <p>Please contact <a href="li.liu@oulu.fi">Li Liu</a> if you have question. The webpage template is by the courtesy of awesome <a href="https://gkioxari.github.io/">Georgia</a>.</p>
</div>
 
<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
